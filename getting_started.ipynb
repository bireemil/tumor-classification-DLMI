{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ebc42a9b-4c32-4385-97bf-81de4e1ed35b",
      "metadata": {
        "id": "ebc42a9b-4c32-4385-97bf-81de4e1ed35b"
      },
      "source": [
        "# Kaggle histopathology introduction\n",
        "This notebook is an introduction to the data challenge of out of distribution classification of histopathology patches. It also serves as a baseline for the code and the model.\n",
        "\n",
        "If you have any questions, feel free to contact me at [leo.fillioux@centralesupelec.fr](mailto:leo.fillioux@centralesupelec.fr)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63498aaa-a8fd-4c8d-92f2-1054c567f3b5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-18T18:44:22.521525Z",
          "iopub.status.busy": "2025-03-18T18:44:22.521216Z",
          "iopub.status.idle": "2025-03-18T18:44:34.914117Z",
          "shell.execute_reply": "2025-03-18T18:44:34.913079Z",
          "shell.execute_reply.started": "2025-03-18T18:44:22.521502Z"
        },
        "id": "63498aaa-a8fd-4c8d-92f2-1054c567f3b5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchmetrics\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import random\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb42acc9-c3fd-4e42-83c3-9b231f119ab9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-18T18:44:39.380135Z",
          "iopub.status.busy": "2025-03-18T18:44:39.379545Z",
          "iopub.status.idle": "2025-03-18T18:44:39.384531Z",
          "shell.execute_reply": "2025-03-18T18:44:39.383564Z",
          "shell.execute_reply.started": "2025-03-18T18:44:39.380101Z"
        },
        "id": "eb42acc9-c3fd-4e42-83c3-9b231f119ab9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# BASE_PATH = \"/kaggle/input/mva-dlmi-2025-histopathology-ood-classification\"\n",
        "TRAIN_IMAGES_PATH = 'data/train.h5'\n",
        "VAL_IMAGES_PATH = 'data/val.h5'\n",
        "TEST_IMAGES_PATH = 'data/test.h5'\n",
        "SEED = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bbe3356-6c15-43b0-80b4-fae3ea7c0951",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-18T18:44:40.910147Z",
          "iopub.status.busy": "2025-03-18T18:44:40.909776Z",
          "iopub.status.idle": "2025-03-18T18:44:40.919446Z",
          "shell.execute_reply": "2025-03-18T18:44:40.918445Z",
          "shell.execute_reply.started": "2025-03-18T18:44:40.910120Z"
        },
        "id": "1bbe3356-6c15-43b0-80b4-fae3ea7c0951",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "torch.random.manual_seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7a5c8b8-ac77-4c9c-b05c-a1b7891fcf9d",
      "metadata": {
        "id": "f7a5c8b8-ac77-4c9c-b05c-a1b7891fcf9d"
      },
      "source": [
        "## 1. Introduction to the data\n",
        "The dataset consists of patches of whole slide images which should be classified into either containing tumor or not. The training images come from 3 different centers (i.e. hospitals), while the validation set comes from another center and the test set from yet another center. The visual aspect of the patches are quite different due to the slightly different staining procedures, conditions, and equipment from each hospital. The objective of the task is to build a classifier that is impacted by this distribution shift as little as possible.\n",
        "\n",
        "The data is stored in `.h5` files, which can be seen as a folder hierarchy, which are can be seen as the following.\n",
        "```\n",
        "├── idx           # index of the image\n",
        "│   └── img       # image in a tensor format\n",
        "│   └── label     # binary label of the image\n",
        "│   └── metadata  # some metadata on the images\n",
        "```\n",
        "The metadata is included for completeness but is not necessarily useful. The first element in the metadata corresponds to the center.\n",
        "\n",
        "The following is a visualization of how different the images look from the different centers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3f4d3ca-47a4-470f-be48-430af587df90",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-18T18:44:43.514807Z",
          "iopub.status.busy": "2025-03-18T18:44:43.514482Z",
          "iopub.status.idle": "2025-03-18T18:44:43.519397Z",
          "shell.execute_reply": "2025-03-18T18:44:43.518298Z",
          "shell.execute_reply.started": "2025-03-18T18:44:43.514782Z"
        },
        "id": "b3f4d3ca-47a4-470f-be48-430af587df90",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_images = {0: {0: None, 1: None},\n",
        "                3: {0: None, 1: None},\n",
        "                4: {0: None, 1: None}}\n",
        "val_images = {1: {0: None, 1: None}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3d7ab90-33ce-48ec-9404-4bce72702e27",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-18T18:44:45.204572Z",
          "iopub.status.busy": "2025-03-18T18:44:45.204257Z"
        },
        "id": "d3d7ab90-33ce-48ec-9404-4bce72702e27",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for img_data, data_path in zip([train_images, val_images], [TRAIN_IMAGES_PATH, VAL_IMAGES_PATH]):\n",
        "    with h5py.File(data_path, 'r') as hdf:\n",
        "        for img_idx in list(hdf.keys()):\n",
        "            label = int(np.array(hdf.get(img_idx).get('label')))\n",
        "            center = int(np.array(hdf.get(img_idx).get('metadata'))[0])\n",
        "            if img_data[center][label] is None:\n",
        "                img_data[center][label] = np.array(hdf.get(img_idx).get('img'))\n",
        "            if all(all(value is not None for value in inner_dict.values()) for inner_dict in img_data.values()):\n",
        "                break\n",
        "all_data = {**train_images, **val_images}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14e76c4d-7d3e-4db2-b43a-03c4c310b0fc",
      "metadata": {
        "id": "14e76c4d-7d3e-4db2-b43a-03c4c310b0fc"
      },
      "source": [
        "## 2. Building a baseline model\n",
        "The baseline model consists of extracting DINOv2 embeddings and linear probing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d73dce1-2b5a-4e77-b2e3-8919a38fb78f",
      "metadata": {
        "id": "2d73dce1-2b5a-4e77-b2e3-8919a38fb78f"
      },
      "source": [
        "### 2.1. Baseline dataset\n",
        "We start by creating the model to read and process the data. For this simple model we also use another dataset with the preprocessed embeddings to avoid recomputing the same embeddings each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc7e9b17-a032-4c7c-84b6-0df111f20774",
      "metadata": {
        "id": "fc7e9b17-a032-4c7c-84b6-0df111f20774"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import h5py\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BaselineDataset(Dataset):\n",
        "    def __init__(self, dataset_path, preprocessing=None, mode=\"train\", augmentations=None, num_augmentations=1):\n",
        "        super(BaselineDataset, self).__init__()\n",
        "        self.dataset_path = dataset_path\n",
        "        self.preprocessing = preprocessing\n",
        "        self.mode = mode\n",
        "        self.augmentations = augmentations\n",
        "        self.num_augmentations = num_augmentations if mode == \"train\" else 1  # Only augment during training\n",
        "\n",
        "        with h5py.File(self.dataset_path, 'r') as hdf:\n",
        "            self.image_ids = list(hdf.keys())  # Original image IDs\n",
        "\n",
        "        # Expand dataset size\n",
        "        self.expanded_indices = [\n",
        "            (idx, aug_idx) for idx in range(len(self.image_ids)) for aug_idx in range(self.num_augmentations)\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.expanded_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_idx, aug_idx = self.expanded_indices[idx]\n",
        "        img_id = self.image_ids[img_idx]\n",
        "\n",
        "        with h5py.File(self.dataset_path, 'r') as hdf:\n",
        "            img = torch.tensor(np.array(hdf.get(img_id).get('img')))\n",
        "            label = np.array(hdf.get(img_id).get('label')) if self.mode == 'train' else -1\n",
        "\n",
        "        # Apply standard preprocessing\n",
        "        if self.preprocessing is not None:\n",
        "            img = self.preprocessing(img)\n",
        "\n",
        "        # Apply augmentation\n",
        "        if self.augmentations is not None and self.mode == \"train\":\n",
        "            img = self.augmentations(img)\n",
        "\n",
        "        return img.float(), torch.tensor(label, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1948f5e9-3ff2-46d4-9cef-b9c641c908c2",
      "metadata": {
        "id": "1948f5e9-3ff2-46d4-9cef-b9c641c908c2"
      },
      "outputs": [],
      "source": [
        "def precompute(dataloader, model, device):\n",
        "    xs, ys = [], []\n",
        "    for x, y in tqdm(dataloader, leave=False):\n",
        "        with torch.no_grad():\n",
        "            xs.append(model(x.to(device)).detach().cpu().numpy())\n",
        "        ys.append(y.numpy())\n",
        "    xs = np.vstack(xs)\n",
        "    ys = np.hstack(ys)\n",
        "    return torch.tensor(xs), torch.tensor(ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d82d0da0-1632-49b7-8b56-6b0e7212f318",
      "metadata": {
        "id": "d82d0da0-1632-49b7-8b56-6b0e7212f318"
      },
      "outputs": [],
      "source": [
        "class PrecomputedDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        super(PrecomputedDataset, self).__init__()\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx].float()\n",
        "\n",
        "    def save(self, file_path):\n",
        "        \"\"\"Save the dataset to a file.\"\"\"\n",
        "        torch.save({'features': self.features, 'labels': self.labels}, file_path)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file_path):\n",
        "        \"\"\"Load the dataset from a file and return a new instance.\"\"\"\n",
        "        data = torch.load(file_path)\n",
        "        return cls(data['features'], data['labels'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b4af3ac",
      "metadata": {
        "id": "1b4af3ac"
      },
      "source": [
        "### 2.1.1 Checking the dataset distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f45ebbf9",
      "metadata": {
        "id": "f45ebbf9"
      },
      "outputs": [],
      "source": [
        "class AddGaussianNoise(torch.nn.Module):\n",
        "    def __init__(self, mean=0.0, std=0.1):\n",
        "        super().__init__()\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def forward(self, tensor):\n",
        "        noise = torch.randn_like(tensor) * self.std + self.mean\n",
        "        return torch.clamp(tensor + noise, 0.0, 1.0)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}(mean={self.mean}, std={self.std})\"\n",
        "\n",
        "import torch.nn.functional as Fnn\n",
        "import torch\n",
        "\n",
        "class Sharpen(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        x = x.float() / 255.0 if x.max() > 1 else x.float()  # normalize if needed\n",
        "        kernel = torch.tensor([[[[0, -1,  0],\n",
        "                                 [-1,  5, -1],\n",
        "                                 [0, -1,  0]]]], dtype=torch.float32, device=x.device)\n",
        "        kernel = kernel.repeat(x.shape[0], 1, 1, 1)\n",
        "        x = x.unsqueeze(0) if x.dim() == 3 else x\n",
        "        x = Fnn.conv2d(x, kernel, padding=1, groups=x.shape[1]).squeeze(0)\n",
        "        return torch.clamp(x, 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dae3866b-938c-4f24-9f0c-3a3a4d9db54e",
      "metadata": {
        "id": "dae3866b-938c-4f24-9f0c-3a3a4d9db54e",
        "outputId": "fccb11a7-1aee-4b0e-cf72-bf4031542f4e"
      },
      "outputs": [],
      "source": [
        "train_augmentations = transforms.Compose([\n",
        "    # transforms.ToTensor(),\n",
        "    transforms.RandomHorizontalFlip(p=0.2),\n",
        "    # transforms.RandomAffine(3, shear=0.05),\n",
        "    transforms.RandomAdjustSharpness(2),\n",
        "    transforms.RandomRotation(5),\n",
        "    transforms.RandomApply([\n",
        "        transforms.ColorJitter(brightness=[0.2,1.05], saturation=[0.8,2], contrast=[0.7,1.6], hue=0.1)], p=0.5),\n",
        "    transforms.RandomGrayscale(p=0.1),\n",
        "    transforms.RandomApply([AddGaussianNoise(mean=0.0, std=0.1)], p=0.5)\n",
        "])\n",
        "\n",
        "train_preprocessing = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    Sharpen()\n",
        "])\n",
        "\n",
        "train_set = BaselineDataset(TRAIN_IMAGES_PATH, preprocessing=train_preprocessing,\n",
        "                                mode='train', augmentations=train_augmentations, num_augmentations=1)\n",
        "\n",
        "# train_set,_ = random_split(train_set, [25000, len(train_set) - 25000])\n",
        "\n",
        "val_set = BaselineDataset(VAL_IMAGES_PATH, preprocessing=train_preprocessing, mode='train')\n",
        "# val_set,_ = random_split(val_set, [10000, len(val_set) - 10000])\n",
        "\n",
        "# print(f\"Original training images: {len(train_set.image_ids)}\")\n",
        "print(f\"Total augmented training images: {len(train_set)}\")  # Should be 3× original size if num_augmentations=3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28b35508",
      "metadata": {
        "id": "28b35508",
        "outputId": "82d2f4a5-eda1-4c60-a79b-87293b88ed3b"
      },
      "outputs": [],
      "source": [
        "fig,axs = plt.subplots(6,6, figsize=(15,15))\n",
        "for k,i in enumerate(np.random.choice(np.arange(len(train_set)), size = len(axs.flatten()), replace = False)):\n",
        "    img, label = train_set.__getitem__(i)\n",
        "    img = (img - img.min())/(img.max() - img.min())\n",
        "    # print(np.moveaxis(img.numpy(), 0, -1))\n",
        "    axs.flatten()[k].imshow(np.moveaxis(img.numpy(), 0, -1).astype(np.float32))\n",
        "    axs.flatten()[k].set_title(str(label.item()))\n",
        "    axs.flatten()[k].axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0696aad",
      "metadata": {
        "id": "e0696aad",
        "outputId": "61f3f849-32c6-4b6f-d06a-d0d964ec56b5"
      },
      "outputs": [],
      "source": [
        "fig,axs = plt.subplots(6,6, figsize=(15,15))\n",
        "for k,i in enumerate(np.random.choice(np.arange(len(val_set)), size = len(axs.flatten()), replace = False)):\n",
        "    img, label = val_set.__getitem__(i)\n",
        "    img = (img - img.min())/(img.max() - img.min())\n",
        "    # print(np.moveaxis(img.numpy(), 0, -1))\n",
        "    axs.flatten()[k].imshow(np.moveaxis(img.numpy(), 0, -1).astype(np.float32))\n",
        "    axs.flatten()[k].set_title(str(label.item()))\n",
        "    axs.flatten()[k].axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ec43b5-32a8-4a02-8794-28f33291133e",
      "metadata": {
        "id": "22ec43b5-32a8-4a02-8794-28f33291133e"
      },
      "source": [
        "### 2.2. Building the models and precomputing the features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb85ba49",
      "metadata": {
        "id": "bb85ba49"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2936e5f2-8702-4b52-987a-02765194f261",
      "metadata": {
        "id": "2936e5f2-8702-4b52-987a-02765194f261",
        "outputId": "853ee93f-4f48-4d95-85ec-1b92c43e6a28"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Working on {device}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c622bc89-5505-4cb7-b38b-ed86b081b8f9",
      "metadata": {
        "id": "c622bc89-5505-4cb7-b38b-ed86b081b8f9",
        "outputId": "86cc4c5d-bbea-46a3-ced5-f01e5ccf7b45"
      },
      "outputs": [],
      "source": [
        "feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14').to(device)\n",
        "# feature_extractor.eval()\n",
        "print(feature_extractor.num_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e015134",
      "metadata": {
        "id": "3e015134"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class TransferModel(nn.Module):\n",
        "    def __init__(self, feature_extractor = feature_extractor, finetuning = True):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = feature_extractor\n",
        "        for name, param in self.feature_extractor.named_parameters():\n",
        "            if \"blocks.0\" in name or \"blocks.1\" in name:  # freeze first few layers\n",
        "                param.requires_grad = False\n",
        "                \n",
        "        self.linear_probing = nn.Sequential(\n",
        "                            nn.Linear(feature_extractor.num_features, 512),\n",
        "                            nn.ReLU(),\n",
        "                            nn.BatchNorm1d(512),\n",
        "                            nn.Dropout(0.5),  # Increased dropout\n",
        "                            nn.Linear(512, 256),\n",
        "                            nn.ReLU(),\n",
        "                            nn.BatchNorm1d(256),\n",
        "                            nn.Dropout(0.3),\n",
        "                            nn.Linear(256, 1),\n",
        "                            nn.Sigmoid()\n",
        "                        ).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_probing(self.feature_extractor(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "901a3bba-b7cc-4a7c-80b3-a4dfe5170429",
      "metadata": {
        "id": "901a3bba-b7cc-4a7c-80b3-a4dfe5170429"
      },
      "outputs": [],
      "source": [
        "# train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# train_dataset = PrecomputedDataset(*precompute(train_dataloader, feature_extractor, device))\n",
        "# val_dataset = PrecomputedDataset(*precompute(val_dataloader, feature_extractor, device))\n",
        "# train_dataset.save(\"augmented_train_dataset.pth\")\n",
        "# val_dataset.save(\"val_dataset.pth\")\n",
        "\n",
        "# train_set = PrecomputedDataset.load(\"augmented_train_dataset.pth\")\n",
        "# val_set = PrecomputedDataset.load(\"val_dataset.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "711f6466-7cc2-40e7-b596-2150155d59c5",
      "metadata": {
        "id": "711f6466-7cc2-40e7-b596-2150155d59c5"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle = True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle = False, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7b9f30c-12da-4514-8550-f86209e34efb",
      "metadata": {
        "id": "e7b9f30c-12da-4514-8550-f86209e34efb"
      },
      "source": [
        "## 3. Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e1f1ba0-1505-41d0-8a25-0d84715702bb",
      "metadata": {
        "id": "9e1f1ba0-1505-41d0-8a25-0d84715702bb"
      },
      "outputs": [],
      "source": [
        "OPTIMIZER = 'Adam'\n",
        "OPTIMIZER_PARAMS = {'lr': 0.0000005} #, \"weight_decay\":0.002}\n",
        "LOSS = 'BCELoss'\n",
        "METRIC = 'Accuracy'\n",
        "NUM_EPOCHS = 500\n",
        "PATIENCE = 50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4928da8",
      "metadata": {
        "id": "a4928da8"
      },
      "outputs": [],
      "source": [
        "# import torch.nn as nn\n",
        "# linear_probing = nn.Sequential(\n",
        "#     nn.Linear(feature_extractor.num_features, 512),\n",
        "#     nn.ReLU(),\n",
        "#     nn.BatchNorm1d(512),\n",
        "#     nn.Dropout(0.5),  # Increased dropout\n",
        "#     nn.Linear(512, 256),\n",
        "#     nn.ReLU(),\n",
        "#     nn.BatchNorm1d(256),\n",
        "#     nn.Dropout(0.3),\n",
        "#     nn.Linear(256, 1),\n",
        "#     nn.Sigmoid()\n",
        "# ).to(device)\n",
        "\n",
        "# filters = [32,64,128]\n",
        "# unet_model = UNet(in_channels=3,filters = filters, depth = len(filters) -1).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92a5338a",
      "metadata": {
        "id": "92a5338a"
      },
      "outputs": [],
      "source": [
        "model = TransferModel(feature_extractor = feature_extractor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df907b9-fe6c-41a5-b8e0-236fb4b9632a",
      "metadata": {
        "id": "8df907b9-fe6c-41a5-b8e0-236fb4b9632a"
      },
      "outputs": [],
      "source": [
        "optimizer = getattr(torch.optim, OPTIMIZER)(model.parameters(), **OPTIMIZER_PARAMS)\n",
        "# optimizer = getattr(torch.optim, OPTIMIZER)(unet_model.parameters(), **OPTIMIZER_PARAMS)\n",
        "criterion = getattr(torch.nn, LOSS)()\n",
        "metric = getattr(torchmetrics, METRIC)('binary')\n",
        "min_loss, best_epoch = float('inf'), 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b675c2d",
      "metadata": {
        "id": "4b675c2d"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history, save_file=None):\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    full_val_epochs = range(1, len(history['val_loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, history['train_loss'], label='Train Loss', linestyle='-')\n",
        "    plt.plot(full_val_epochs, history['val_loss'], label='Validation Loss', linestyle='-', color=\"C1\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Metric plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, history['train_metric'], label='Train Metric', linestyle='-')\n",
        "    plt.plot(full_val_epochs, history['val_metric'], label='Validation Metric', linestyle='-', color=\"C1\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Metric')\n",
        "    plt.title('Training and Validation Metric')\n",
        "    plt.legend()\n",
        "    if save_file:\n",
        "        plt.savefig(save_file)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5fb3925-2f52-47ec-b46c-9890f4965714",
      "metadata": {
        "id": "a5fb3925-2f52-47ec-b46c-9890f4965714",
        "outputId": "4242293f-15cd-4f7c-a628-584e40f61698"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# Assuming history dict to store loss and metric values\n",
        "history = {'train_loss': [], 'train_metric': [], 'val_loss': [], 'val_metric': []}\n",
        "\n",
        "accumulation_steps = 5\n",
        "epoch_bar = tqdm(range(NUM_EPOCHS), leave=True, desc='Training Progress')\n",
        "\n",
        "for epoch in epoch_bar:\n",
        "    model.train()\n",
        "    train_metrics, train_losses = [], []\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, (train_x, train_y) in enumerate(train_loader):\n",
        "        print(f\"{i / len(train_loader) * 100:.2f}%\", end=\"\\r\")\n",
        "\n",
        "        # Forward pass\n",
        "        train_x, train_y = train_x.to(device), train_y.to(device)\n",
        "        augmented_images = [train_x]  # Add more augmentations if needed\n",
        "        original_pred = model(augmented_images[0]).squeeze(1)\n",
        "        task_loss = criterion(original_pred.float(), train_y.float())\n",
        "        total_loss = task_loss / accumulation_steps  # normalize loss\n",
        "\n",
        "        # Backward pass (accumulating gradients)\n",
        "        total_loss.backward()\n",
        "\n",
        "        # Step every N mini-batches\n",
        "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        train_losses.extend([task_loss.item()] * len(train_y))  # Note: use `task_loss`, not scaled one\n",
        "\n",
        "        train_metric = metric(original_pred.cpu(), train_y.int().cpu())\n",
        "        train_metrics.extend([train_metric.item()] * len(train_y))\n",
        "\n",
        "    train_loss_mean = np.mean(train_losses)\n",
        "    train_metric_mean = np.mean(train_metrics)\n",
        "    history['train_loss'].append(train_loss_mean)\n",
        "    history['train_metric'].append(train_metric_mean)\n",
        "\n",
        "    model.eval()\n",
        "    val_metrics, val_losses = [], []\n",
        "\n",
        "    for val_x, val_y in val_loader:\n",
        "        with torch.no_grad():\n",
        "            # Apply random augmentations to the validation image\n",
        "            augmented_images = [val_x.to(device)]  # Start with the original image\n",
        "\n",
        "            # Generate augmented images by applying random transformations\n",
        "            # for _ in range(10):\n",
        "            #     augmented_images.append(augmentation_transforms(val_x.to(device)))\n",
        "\n",
        "            # Make predictions on the original image and the augmented images\n",
        "            original_pred = model(augmented_images[0]).squeeze(1)\n",
        "            augmented_preds = [model(img).squeeze(1) for img in augmented_images[1:]]\n",
        "\n",
        "            # Combine predictions (e.g., averaging them)\n",
        "            all_preds = [original_pred] + augmented_preds\n",
        "            ensemble_pred = torch.mean(torch.stack(all_preds), dim=0)  # Averaging the predictions\n",
        "\n",
        "            # Loss (you could use cross-entropy, MSE, etc.)\n",
        "            loss = criterion(ensemble_pred.float(), val_y.to(device).float())\n",
        "            val_losses.extend([loss.item()] * len(val_y))\n",
        "\n",
        "            # Calculate the metric (e.g., accuracy, F1-score)\n",
        "            val_metric = metric(ensemble_pred.cpu(), val_y.int().cpu())\n",
        "            val_metrics.extend([val_metric.item()] * len(val_y))\n",
        "\n",
        "    val_loss_mean = np.mean(val_losses)\n",
        "    val_metric_mean = np.mean(val_metrics)\n",
        "\n",
        "    history['val_loss'].append(val_loss_mean)\n",
        "    history['val_metric'].append(val_metric_mean)\n",
        "\n",
        "    epoch_bar.set_postfix({\n",
        "        'Train Loss': f'{train_loss_mean:.3g}',\n",
        "        f'Train {METRIC}': f'{train_metric_mean:.3g}',\n",
        "        'Val Loss': f'{val_loss_mean:.3g}',\n",
        "        f'Val {METRIC}': f'{val_metric_mean:.3g}',\n",
        "    })\n",
        "\n",
        "    if val_loss_mean < min_loss:\n",
        "        min_loss = val_loss_mean\n",
        "        best_epoch = epoch\n",
        "        torch.save(model.state_dict(), 'best_model_bis.pth')\n",
        "\n",
        "    if epoch - best_epoch == PATIENCE:\n",
        "        break\n",
        "\n",
        "    plot_training_history(history, \"output.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b683901",
      "metadata": {
        "id": "5b683901",
        "outputId": "71bf6c6c-a44a-4f69-f844-eaa668341f20"
      },
      "outputs": [],
      "source": [
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6c7a540-f9ab-414d-8ea9-29efb9fa18ee",
      "metadata": {
        "id": "d6c7a540-f9ab-414d-8ea9-29efb9fa18ee"
      },
      "source": [
        "## 4. Making the final prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0d9748f-363f-41c9-8128-a2fa8ce47494",
      "metadata": {
        "id": "e0d9748f-363f-41c9-8128-a2fa8ce47494"
      },
      "source": [
        "To create a solutions file, you need to generate a CSV with 2 columns.\n",
        "- **ID**: containing the ID of the image\n",
        "- **Pred**: with the predicted class (**threshold the prediction to get either 0 or 1**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a4a172e-0773-489d-a9b3-fab4033efab6",
      "metadata": {
        "id": "3a4a172e-0773-489d-a9b3-fab4033efab6"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"best_model_bis.pth\", weights_only=True))\n",
        "model.eval()\n",
        "model.to(device)\n",
        "prediction_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c57d359",
      "metadata": {
        "id": "3c57d359"
      },
      "outputs": [],
      "source": [
        "# test_preprocessing = transforms.Compose([transforms.Resize((98,98)),\n",
        "#                                          transforms.Normalize(mean =[0.673, 0.483, 0.739],\n",
        "#                                                               std = [0.194, 0.222, 0.123])])\n",
        "# test_dataset = BaselineDataset(TEST_IMAGES_PATH, preprocessing = train_preprocessing, mode = \"eval\")\n",
        "# test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=256)\n",
        "# get_channel_histograms(test_dataloader)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bb34fa3-bb12-4ba0-86ae-8be8eaa71a62",
      "metadata": {
        "id": "0bb34fa3-bb12-4ba0-86ae-8be8eaa71a62"
      },
      "outputs": [],
      "source": [
        "with h5py.File(TEST_IMAGES_PATH, 'r') as hdf:\n",
        "    test_ids = list(hdf.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aebd132-67ba-4623-b69e-8df715533a2c",
      "metadata": {
        "id": "6aebd132-67ba-4623-b69e-8df715533a2c",
        "outputId": "bd10eb8b-7115-4c51-8255-45a17b0c4e94"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import h5py\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define a set of possible transformations for augmentation during test inference\n",
        "augmentation_transforms = T.Compose([\n",
        "    T.RandomChoice([\n",
        "        T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "        T.Grayscale(num_output_channels=3),  # Convert to grayscale with 3 channels (RGB)\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.RandomVerticalFlip(),\n",
        "    ])\n",
        "])\n",
        "\n",
        "# Number of augmented images to use for ensemble during testing\n",
        "num_augmented_images = 0\n",
        "\n",
        "solutions_data = {'ID': [], 'Pred': []}\n",
        "\n",
        "with h5py.File(TEST_IMAGES_PATH, 'r') as hdf:\n",
        "    for test_id in tqdm(test_ids):\n",
        "        # Preprocess the original image\n",
        "        img = train_preprocessing(torch.tensor(np.array(hdf.get(test_id).get('img')))).unsqueeze(0).float()\n",
        "\n",
        "        # Apply random augmentations to the original image\n",
        "        augmented_images = [img.to(device)]  # Start with the original image\n",
        "\n",
        "        # Generate augmented images by applying random transformations\n",
        "        # for _ in range(num_augmented_images):\n",
        "        #     augmented_images.append(augmentation_transforms(img.to(device)))\n",
        "\n",
        "        # Make predictions on the original image and the augmented images\n",
        "        original_pred = model(augmented_images[0]).detach().cpu()\n",
        "        augmented_preds = [model(img).detach().cpu() for img in augmented_images[1:]]\n",
        "\n",
        "        # Combine predictions (e.g., averaging them)\n",
        "        all_preds = [original_pred] + augmented_preds\n",
        "        ensemble_pred = torch.mean(torch.stack(all_preds), dim=0)  # Averaging the predictions\n",
        "\n",
        "        # Get the final prediction by thresholding the ensemble prediction\n",
        "        final_pred = int(ensemble_pred.item() > 0.5)\n",
        "\n",
        "        # Store the ID and the final prediction\n",
        "        solutions_data['ID'].append(int(test_id))\n",
        "        solutions_data['Pred'].append(final_pred)\n",
        "\n",
        "# Convert to a DataFrame and save to CSV\n",
        "solutions_data = pd.DataFrame(solutions_data).set_index('ID')\n",
        "solutions_data.to_csv('model_test.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf46202",
      "metadata": {
        "id": "acf46202"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5adaf09",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 11165379,
          "sourceId": 93787,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
